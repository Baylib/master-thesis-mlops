\section{MLOps}\label{sec:mlops}

In this section we will try to describe key aspect of MLOps as can be found in today's literature.

\subsection{Description}\label{subsec:description}
MLOps or Machine Learning Operations is the organisation and automation of Machine learning development workflow and operations.
It's an extension of DevOps practices adapted to machine learning model development and data operations\cite{Kreuzberger2022MachineLO}.

MLOps is build upon DevOps, Data Engineering and Machine Learning as stated by\cite{9792270}.

\begin{figure}[!htbp]
    \caption{MLOps field\cite{9792270}}
    \centering
    \includegraphics[scale=0.5]{images/ml-dev-data-ops}
    \label{fig:ml-dev-data-ops}
\end{figure}


MLOps with clearly defined workflow and infrastructure can provide transparency and accountability to Upper Management and Business.\cite{treveil2020introducing}(p.11)

\begin{figure}[!htbp]
    \caption{MLOps lifecycle\cite{mlops-definition-tools-and-challenge}}
    \centering
    \includegraphics[scale=0.5]{images/ml-dev-ops}
    \label{fig:ml-dev-ops}
\end{figure}

MLOps is essential for data teams and organisations, as it involves a complex integration of technologies, processes, and people, requiring ongoing discipline and time to implement effectively.
It is not a one-time task but a continuous effort to ensure successful machine learning operations\cite{treveil2020introducing}(p.38).

A survey in 2021\cite{DBLP:journals/corr/abs-2103-08942} already show that Machine Learning development was evolving towards team-based development.
They also show that in a result the adoption of DevOps practices is increasing and the required skills in infrastructure and deployment are evolving.

In this regard, they also identified the following obstacles among others\cite{DBLP:journals/corr/abs-2103-08942}:
    \begin{itemize}
        \item Accessibility of data
        \item Building training pipelines
        \item Deployment and target environment (on-premise, cloud)
        \item Tracking and comparing training
        \item Collaborating on projects
        \item Lack of guidelines
    \end{itemize}

Since then, we will see that some of those difficulties have been tackled, tools exist and are currently reviewed in the literature.

In 2025, in their systematic literature review\cite{10855428}, they still consider MLOps as an emerging field.
But they admit, publications are still limited.
Variations in defining and grouping steps for creating and maintaining models arise from differing interpretations,
model architectures, and usage methods, leading to diverse implementation approaches.

\subsection{Challenges}\label{subsec:challenges}

Adopting MLOps presents challenges, as highlighted by\cite{Kreuzberger2022MachineLO}, which categorizes them into organizational,
ML system, and operational issues.
Organizational challenges include shifting to a product-oriented mindset, addressing skill shortages,
and fostering multidisciplinary teamwork, while ML system challenges focus on scalability and fluctuating demand,
and operational challenges emphasize automation, governance, and reproducibility.
These challenges are closely tied to the deployment stage in ML workflows, which involves critical steps like data management,
model training, and deployment, each accompanied by issues such as data dispersion, resource constraints, and also ethical concerns\cite{10.1145/3533378}.
All together, these challenges and considerations mark the complexity of implementing MLOps effectively.

In their publication, a Microsoft team\cite{8804457} outlined challenges for MLOps implementation, including Data Availability, Collection, Cleaning, and Management,
Education and Training, and Hardware Resources.
They also highlighted the need for End-to-end Pipeline Support, Model Evolution, Evaluation, and Deployment, and effective Collaboration and Working Culture.
Additionally, challenges like Integrating AI into Larger Systems, AI Tools, and Scale were emphasized,
along with the importance of Guidance and Mentoring for skill development.
These challenges highlight the multifaceted nature of MLOps, requiring both technical and organizational solutions.

\subsection{Roles in MLOps}\label{subsec:actors}

Machine learning development has evolved into a collaborative, team-based process involving diverse roles.
Each role, from business experts to technical architects, contributes to the lifecycle of Machine Learning models.
Their responsibilities include ensuring model maintenance, optimization, trust in deployment,
and mitigating business risks\cite{treveil2020introducing}(p.22).

The data scientist is often considered the central role in this process.
However, within the context of MLOps, the literature identifies several key roles that
collaborate to support the development and deployment of machine learning models.
These roles include\cite{treveil2020introducing}(p.14):
Subject-Matter Experts, Data Scientists, Data Engineers, DevOps engineers, Software engineers, Model Risk Managers, Machine Learning architects.
Additional literature further refines these roles, proposing the following\cite{Kreuzberger2022MachineLO}:

\begin{itemize}
    \item Business Stakeholder
    \item Solution Architect
    \item Data Scientist
    \item Data Engineer
    \item Software Engineer
    \item DevOps Engineer
    \item ML Engineer/MLOps Engineer
\end{itemize}

\textit{Each person—from the subject-matter-expert on the business side to the most
technical machine learning architect—plays a critical part in the maintenance of ML
models in production}\cite{treveil2020introducing}(p.22).

In figure\ref{fig:mlop-people}, they propose a picture of the model lifecycle and roles within an organisation.
\begin{figure}[!htbp]
    \caption{Machine learning model life cycle inside an average
    organisation\cite{treveil2020introducing}(p.6)}
    \centering
    \includegraphics[scale=0.3]{images/mlops-people}
    \label{fig:mlop-people}
\end{figure}

\subsection{Workflow}\label{subsec:workflow}

As we already stated establishing and MLOPs workflow is beneficial for enterprise.
In this section we'll try to picture what a good MLOPs workflow should be according to the literature.

In\cite{9792270} They propose a general workflow~\ref{fig:taxo-workflow} that shares many aspect with other workflows like those found in\cite{treveil2020introducing,gift2021practical}
It is high level but shows the initiation process coming from the business requirements going to an automate MLOps workflow.

\begin{figure}[!htbp]
    \caption{Proposed workflow in\cite{9792270}}
    \centering
    \includegraphics[scale=0.3]{images/taxo-workflow}
    \label{fig:taxo-workflow}
\end{figure}

The more complete proposition of an MLOps workflow was found in\cite{Kreuzberger2022MachineLO}.
It split the workflow in 4 phases as shown in figure~\ref{fig:end-to-end-workflow}.:

\begin{itemize}
    \item MLOps project initialization that involves analyzing the business problem, designing the system architecture and selecting technologies, deriving the ML task, identifying required data, and connecting to raw data for initial analysis.
    \item Project initiation with the definition of requirements and implementation of DataOps pipelines
    \item Experimentation were development of the model occurs
    \item Automated MLOps pipelines to prepare for production
\end{itemize}

\begin{figure}[!htbp]
    \caption{MLOps Architecture proposed in \cite{Kreuzberger2022MachineLO}}
    \centering
    \includegraphics[scale=0.5]{images/kreuz-end-to-end-workflow}
    \label{fig:end-to-end-workflow}
\end{figure}

Figure~\ref{fig:mlops-workflow} show a simple MLOps workflow representation that also integrate DataOps.
\begin{figure}[!htbp]
    \caption{MLOps workflow\cite{8804457}}
    \centering
    \includegraphics[scale=0.6]{images/mlops-workflow-9-stages}
    \label{fig:mlops-workflow}
\end{figure}

\subsubsection{Good practices and essential components}

In the review\cite{Kreuzberger2022MachineLO}, they extracted good practices and principles to implement a MLOps workflow.
We complete their list by adding the State-of-the-art MLOps component that should be used for any models as described in~\cite{BURGUENOROMERO2025107499}.

\begin{itemize}
    \item CI/CD automation
    \item Pipeline/Workflow orchestration
    \item Reproducibility using pipeline orchestrator
    \item Versioning of data, code and model with model registry, code repositories and container registries
    \item Collaboration with collaborative development platform (GitHub, Gitlab, Azure Devops)
    \item Continuous ML training and evaluation with well-defined triggers for pipelines
    \item ML metadata tracking
    \item Continuous Monitoring (Monitoring system)
    \item Feedback loops
\end{itemize}

As we explained earlier those principles shares a lot with actual DevOps practices with the addition of ML considerations
like training and metadata tracking.

\subsubsection{Developing Models}
CI/CD principles, commonly used in traditional software engineering, are essential for machine learning systems.
While developing a model, a data scientist should push code, metadata, and documentation to a central repository, triggering a CI/CD pipeline\cite{10855428}.
An example pipeline includes the following steps\cite{treveil2020introducing}(p.74).

\begin{figure}[!htbp]
    \caption{MLOps pipeline\cite{inproceedings}}
    \centering
    \includegraphics[scale=0.5]{images/mlops-pipeline}
    \label{fig:mlops-pipeline}
\end{figure}

\subsubsubsection{Develop the model:}
\begin{itemize}
    \item Create model artifacts
    \item Store artifacts in long-term storage
    \item Run basic checks
    \item Generate fairness and explainability reports
\end{itemize}
\subsubsubsection{Deploy to a Test Environment:}
\begin{itemize}
    \item Validate ML and computational performance
    \item Conduct manual validation
\end{itemize}
\subsubsubsection{Deploy to Production:}
\begin{itemize}
    \item Roll out the model incrementally (e.g., as a canary deployment)
    \item Fully deploy the model once validated
\end{itemize}

\subsubsection{Training}

Model training involves feeding the selected model with a dataset to learn data patterns.
The model training infrastructure provides computational resources (CPUs, RAM, GPUs) in either distributed or non-distributed configurations, with scalable distributed systems.
It's usually implemented through local machines, cloud computing, or worker nodes using frameworks like Kubernetes\cite{Kreuzberger2022MachineLO, Haakman2021}.

Model training primarily occurs in secure on-premises environments, accessing data from connected lakes\cite{Haakman2021}.
Due to strict security policies, some isolated systems prohibit internet connectivity to external cloud services
It requires practitioners to use pre-approved tools from internal repositories, but it ensures data protection for sensitive information\cite{Haakman2021}.

Environmental impact is a concern, as ML training increases energy consumption and CO\textsubscript{2} emissions\cite{10.1145/3533378}.
Privacy is also critical, with risks like membership inference attacks\cite{10.1145/3533378, 10.1145/3555803}.
A major concern is the economic cost due to computational resources, particularly in Natural Language Processing\cite{10.1145/3533378}.

\subsubsubsection{Hyper-parameter Optimization (HPO)}
HPO selects optimal hyper-parameters (e.g., decision tree depth, neural network layers) and often requires multiple training cycles, making it computationally expensive.
Large datasets further complicate HPO by increasing training time per search\cite{10.1145/3533378}.
Defining a complete search space is often impractical due to insufficient problem knowledge, limiting state-of-the-art HPO adoption.

\subsubsection{Testing and Validating}
Model verification ensures generalization, robustness, and compliance with functional requirements.
This process involves three key steps: requirement encoding, formal verification, and test-based verification\cite{10.1145/3533378}.

Beyond accuracy, factors like robustness, security, transparency, fairness, and safety are critical for building reliable and ethical AI\cite{10.1145/3555803}.
Defining domain-specific metrics (e.g., business KPIs) is critical, as proxy metrics (e.g., clicks) may not align with business goals\cite{10.1145/3533378}.
Cross-disciplinary collaboration is essential to establish good metrics.

\subsubsection{Deploy in production}
Maintaining machine learning systems in production presents unique challenges that differ from traditional software systems.
While DevOps principles provide useful foundations for system maintenance, ML systems require specialized approaches due to their distinct characteristics\cite{10.1145/3533378}.

The survey~\cite{10.1145/3533378} review key challenges which include the lack of standardized methods for collecting telemetry data and difficulties in obtaining accurate labels during operation.

These differences have led to the development of specific practices for production ML systems that build upon but extend beyond conventional DevOps methodologies\cite{gift2021practical,10.1145/3533378}.


\subsubsubsection{Deployment Strategies}\label{subsec:deployment-strategies}
govern how software (or ML models) are rolled out in production.
Key concepts include [9, p.77]:

\begin{itemize}
    \item Integration: Merging code changes into a shared repository and running automated tests.
    \item Delivery: Packaging and validating a model for production (e.g., containerization, artifact storage).
    \item Deployment: Releasing the model to target infrastructure (manual or automated).
    \item Continuous Delivery (CD): Ensuring the model is always production-ready (even if deployed manually).
    \item Continuous Deployment (CD): Fully automated releases upon passing tests.
    \item Release: Decoupling deployment from serving traffic (e.g., A/B testing, shadow deployments).
\end{itemize}
Common ML deployment strategies\cite{gift2021practical}:
\begin{itemize}
    \item Blue-Green Deployment: Two identical environments; traffic switches post-validation.
    \item Canary Release: Gradual rollout to a subset of users.
    \item Shadow Mode: New model runs alongside the old one without affecting traffic.
\end{itemize}


\subsubsection{Monitor and FeedBack}

Understanding Model Degradation
Once a machine learning model is trained and deployed in production, there are two
approaches to monitor its performance degradation: ground truth evaluation and
input drift detection.
Understanding the theory behind and limitations of these
approaches is critical to determining the best strategy.\cite{treveil2020introducing}(p.89)

\begin{figure}[!htbp]
    \caption{Continuous delivery for end-to-end machine learning process\cite{treveil2020introducing}(p.95)}
    \centering
    \includegraphics[scale=0.4]{images/feedback-loop-intro}
    \label{fig:feedback-loop-intro}
\end{figure}


As explained in\cite{treveil2020introducing}(p.103), Traditional software is designed to meet fixed specifications and maintains its functionality after deployment.
In contrast, ML models are defined by their statistical performance on specific data, making them prone to performance degradation as data properties change.
Beyond typical software maintenance, ML systems require careful monitoring of performance drift,
with ground truth-based evaluation being essential and drift detection serving as an early warning.
Retraining on new data is the primary solution, though model adjustments are also possible.
Improved models can be validated using shadow scoring or A/B testing to ensure enhanced performance before deployment.

The frequency of model retraining depends on factors like the domain, the cost of retraining, and the potential performance improvement.
Teams must balance these considerations, such as whether the retraining cost justifies the performance gain or if sufficient new data is available to enhance the model.\cite{treveil2020introducing}(p.86)

\subsubsection{AutoML}

AutoML automates the machine learning process, including tasks like data preparation, model creation, hyperparameter tuning, and evaluation,
making it simpler and more accessible.
It is increasingly integrated into MLOps platforms, enabling companies to deploy ML models into production more efficiently.
Major cloud platforms now offer AutoML tools, enhancing productivity and feasibility for cloud-based ML projects.
This combination of AutoML and MLOps is driving broader adoption across industries.\cite{gift2021practical,mlops-definition-tools-and-challenge}

As explained in\cite{mlops-definition-tools-and-challenge} AutoML (Automated Machine Learning) refers to the automation of the various steps involved in creating a machine learning model,
such as data preparation, model selection, hyperparameter adjustment and evaluation.
More and more companies are using it to speed up and simplify the deployment of ML models.
Although less flexible and more resource-intensive than conventional approaches, AutoML facilitates retraining and integrates effectively with modern MLOps systems.
Auto-sklearn, Auto-Keras or Katib are some example of AutoML tools.



\subsection{Architecture, infrastructure and existing platforms}\label{subsec:architecture-infrastructure-and-existing-platforms}
Figure\ref{fig:infra} shows a reference architecture proposed by\cite{10855428} based on their results.
Their proposed solution integrates GitHub Actions, Docker, and Kubernetes for the DevOps component, complemented by a robust ecosystem of tools:
Feast for metadata storage, Mlflow as the model repository, Kubeflow for model orchestration, and Flower for distributed computing.
This combination creates a comprehensive and scalable MLOps framework.

\begin{figure}[!htbp]
    \caption{MLOps reference architecture\cite{10855428}}
    \centering
    \includegraphics[scale=0.35]{images/infrastructure}
    \label{fig:infra}
\end{figure}


The Acumos AI platform\cite{10690392} is an open-source project that propose a platform that allows to hotswap models and deploy
them to wellknown cloud providers (targets like Kubernetes, AWS, Azure or GCP).
By using microservices, they made it easier to swap the models hidden behind a REST API\@.

The Nifi project\cite{10346079} uses the Acumos project to demonstrate how to take an ML model
to production.
The Nifi project implement automatic and reusable ML model training and ML model serving pipelines.

Reusable MLOps is still a novel concept\cite{10690392}.

The OCDL platform\cite{LIU2020704} was built on Kubernetes and other open-source frameworks to provide a scalable solution.
It also integrates with GitHub to enable CI/CD pipelines and delivery, as illustrated in Figure\ref{fig:ocdl}.
OCDL provides an online IDE for model coding, a reusable code template, data access and GPU/CPU computing instances\cite{LIU2020704} .

\begin{figure}[!htbp]
    \caption{The OCDL architecture layers\cite{LIU2020704}}
    \centering
    \includegraphics[scale=0.5]{images/ocdl-architecture}
    \label{fig:ocdl}
\end{figure}

We will use all their remarks and experiment to create our own.



\subsection{Maturity Models}\label{subsec:matutiry-models}
The authors of\cite{mlops-maturity-model}, they propose a maturity model that goes from manual MLOps to fully automated MLOps.
It differs from the Microsoft and the Google MLOps maturity models,
but they all tend to go from manual to fully automated MLOps processes using automated CI/CD and
ML pipelines.\cite{mlops-definition-tools-and-challenge}

In their research\cite{inproceedings} also identifies key motivators for MLOps and proposes three levels of MLOps maturity.
Level 3 being the most sophisticated and recommended for enterprise solutions.

\begin{figure}[!htbp]
    \caption{Maturity levels \cite{mlops-maturity-model}}
    \centering
    \includegraphics[scale=0.5]{images/maturity-levels}
    \label{fig:maturity}
\end{figure}


