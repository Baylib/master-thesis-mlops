\section{DataOps in MLOps Workflows}\label{sec:dataops}
Machine Learning Operations (MLOps) workflows heavily depend on reliable, high-quality data pipelines\cite{dataops-mlops}.
This is where DataOps comes into play as a critical component of any comprehensive MLOps strategy.
DataOps applies DevOps principles to data management, ensuring data important role in ML systems.

\subsection{DataOps Definition and Principles}\label{subsec:dataops-definition}
DataOps is a methodology that applies agile and DevOps principles to data management, fostering better collaboration,
automated workflows, and seamless integration between data engineers and data users\cite{ad-hoc-dataops}.
It represents the alignment of people, process, and technology to enable the rapid, automated, and secure management of data.
The core principles of DataOps center around several key practices.
Automation plays a critical role by streamlining the creation, testing, and deployment of data pipelines.
Quality is maintained through continuous monitoring and validation of data to ensure reliability and accuracy.
Observability provides transparency into data pipeline operations, enabling teams to detect and resolve issues quickly.
Version control is essential for tracking changes in data, schemas, and transformations, ensuring reproducibility and accountability.
Finally, the use of an agile methodology brings iterative, collaborative development practices to data workflows, allowing teams to adapt quickly to changing requirements.

\subsection{DataOps Workflow}\label{subsec:dataops-workflow}
The DataOps framework integrates methodologies from Agile, DevOps, data management and analytics.

\begin{figure}[!htbp]
    \caption{DataOps lifecycle\cite{FANNOUCH2025100321}}
    \centering
    \includegraphics[scale=0.5]{images/dataops-workflow}
    \label{fig:dataops-workflow}
\end{figure}

Below are the key steps derived from a systematic literature review\cite{FANNOUCH2025100321}:

\subsubsection{Data Orchestration}
Data orchestration involves automating the flow of data from its source through various transformation steps to its final destination.
This helps reduce human error and makes the process more efficient.
It's also important to carefully manage how data moves from one stage to the next—for example,
from raw data to cleaned data and then to analysis-ready formats—so that everything flows in the correct order.

\subsubsection{Data Governance}
Good data governance means making sure the data is accurate and trustworthy.
This includes checking and cleaning the data, so it's consistent and correct.
It also means protecting the data through encryption and access controls,
and ensuring that all processes follow legal regulations such as GDPR.

\subsubsection{Continuous Integration and Deployment (CI/CD)}
CI/CD practices help teams work together smoothly by keeping track of changes in both code and data.
They also involve automatically testing pipelines to catch problems early,
which helps maintain quality and reliability as the system evolves.

\subsubsection{Monitoring and Observability}
Monitoring is about checking how the system is performing—measuring things like speed, error rates,
and resource usage.
Observability adds transparency by logging changes and tracking how data is accessed,
which helps with debugging and maintaining trust in the system.


