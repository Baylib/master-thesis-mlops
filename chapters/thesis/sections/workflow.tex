\section{Workflows}\label{sec:workflow}

\subsection{DevOps CI/CD pipelines}\label{subsec:ci/cd-pipelines-and-development-workflow}
As we explained while describing the infrastructure, part of our global workflow can be implemented within pipelines runners definition.
We used GitHub action and the GitHub flow to harmonise the workflow within our different type of git repositories.
GitHub allows us to define template in a single repository that can be then versioned and used in other repositories while hiding the complexity of the defined pipelines.
We also followed the GitHub flow which involve one main branch and features or hotfix branch to favor small and fast release.
As stated before other strategies can be used with an adapted development workflow.

\begin{figure}[!htbp]
    \centering
    \caption{Implementation of the GitHub Flow CI/CD workflow}
    \includegraphics[scale=0.3]{images/project/cicd-workflow-p1}
    \label{fig:icd-workflow-p1}
\end{figure}

There are our development pipelines for any git repositories except for the deployment repository that are only targets.
Artifact that are not deployed but released to a directory follows the same path but are deployed as dependencies within other projects.
Airflow DAGS follows the same development workflow and are release into our previously defined git deployment repositories.

\begin{figure}[!htbp]
    \centering
    \caption{Development Team Contribution activity}
    \includegraphics[scale=0.3]{images/project/cicd-workflow-p2}
    \label{fig:cd-workflow-p2}
\end{figure}


\subsection{DataOps pipelines}\label{subsec:dataops-pipelines}
Inspired by our research in the literature we create a sample DataOps pipeline that can be implemented with specific
operations.

\begin{figure}[!htbp]
    \centering
    \caption{DataOps sample workflow}
    \includegraphics[scale=0.3]{images/project/dataops-workflow}
    \label{fig:dataops-workflow}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \caption{DataOps workflow define in Airflow}
    \includegraphics[scale=0.3]{images/project/dataops-workflow-airflow}
    \label{fig:dataops-workflow-airflow}
\end{figure}

It's defined in our project as an Airflow DAG and can be triggered manually in early stage of the project or
automatically when the project has enough maturity.

\subsection{MLOps pipelines}\label{subsec:mlops-pipelines}
Our MLOps pipelines are developed as Kubeflow pipelines with the possibility to trigger them from an airflow DAG
even in the early stage of the project in ensure the possibility to combined it with the DataOps pipeline to advance towards AutoML.

\begin{figure}[!htbp]
    \centering
    \caption{MLOps workflow in Kubeflow pipelines}
    \includegraphics[scale=0.3]{images/project/mlops-workflow-kubeflow}
    \label{fig:mlops-workflow-kubeflow}
\end{figure}

In our MLOps workflow, we have established a seamless integration between Apache Airflow and Kubeflow Pipelines to
efficiently manage the machine learning lifecycle.
This integration automates the process from data availability to model deployment, ensuring a streamlined and reproducible workflow.

\subsubsection{Data Availability Trigger}
AS explained in previous section, our DataOps pipeline preprocess and prepare the data to release train and test data's.
Airflow then triggers the Kubeflow pipeline to initiate the MLOps workflow.

\subsubsection{Model Training}
The pipeline begins with a training component that utilizes the provided datasets to train a machine learning model.
The trained model is saved to MinIO, our S3-compatible object storage integrated with Kubeflow.

\subsubsection{Model Validation}
After training, a validation component assesses the model's performance using the test data.
If the model meets our predefined accuracy thresholds and/or performs better than previous models,
the workflow proceeds; otherwise, it terminates, ensuring only validated models advance.

\subsubsection{Release and Deployment}
Upon successful validation, the pipeline enters the release phase, where the model and its metadata are saved to MinIO.
The deployment component updates our production environment with the new model, ensuring minimal disruption and continuous service delivery.

