\chapter{Thesis project: A standard MLOps/GitOps workflow}\label{ch:thesis-project:-a-standard-mlops-ci/cd-workflow}
\chaptermark{Thesis : MLOps/GitOps WorkFlow}
\section{Introduction}\label{sec:introduction}
Building upon our state-of-the-art review of MLOps practices, this chapter introduces our practical implementation of a standardized MLOps workflow.
The proposed framework is designed with versatility in mindâ€”capable of enhancing existing projects at various maturity levels or providing a robust foundation for new initiatives seeking accelerated production deployment.
Rather than developing yet another platform, our approach integrates established tools and workflows identified in the literature into a cohesive system tailored to address modern machine learning development challenges.
A distinguishing feature of our implementation is the incorporation of GitOps principles into the MLOps workflow, creating a synergistic relationship between these complementary methodologies.
At the core of our design is the GitOps philosophy, which establishes Git repositories as the single source of truth for code, configurations, and infrastructure.
We leverage GitHub repositories as the central foundation, with GitHub Actions orchestrating our DevOps workflows.
This integration extends further through tools such as ArgoCD, GitSync, and custom scripts that trigger our DataOps and MLOps pipelines in a synchronized manner.

Our infrastructure is deployable across one or multiple Kubernetes clusters, depending on specific project requirements.
This architecture decouples the workflow definition from hardware considerations, offering significant flexibility.
The Kubernetes implementation supports dedicated nodes with specialized resources (high CPU, memory, GPU) for compute-intensive machine learning or data processing operations.
Furthermore, this approach accommodates diverse deployment scenarios:


\begin{itemize}

\item Cloud-based Kubernetes clusters

\item On-premises infrastructure

\item Single-node Kubernetes configurations running on development laptops (within hardware constraints)

\end{itemize}

The seamless integration of DevOps, DataOps, and MLOps pipelines culminates in a comprehensive MLOps workflow that is portable across any Kubernetes environment. This integration addresses the full machine learning lifecycle from development to deployment while maintaining consistent practices throughout.


In the following sections, we detail the specific tools selected to implement our workflow framework and explain how they interconnect to create a modular system that can adapt as projects evolve in complexity and maturity.



\section{Tools}\label{sec:tools2}
In this section we will list all the tools we picked from our previous research.
As our workflow should be standard it should remain tools agnostic.

\subsection{Git Repositories and CI/CD}\label{subsec:github}
GitHub will be used to store and version control our codes repositories and configurations.
It will also be used to implement our CI/CD pipelines with GitHub actions.

\subsection{IAC, Containers and Registry}\label{subsec:dockerhub}
To benefit from containerization we will use docker to containerize our applications and models.
DockerHub will be used to store and version our Docker containers and Helm charts.

\subsection{Kubernetes}\label{subsec:kubernetes}
We will use a Kubernetes cluster to take advantage of its orchestration capabilities, advanced deployment strategies,
and the ecosystem of tools available on the platform.

\subsection{Airflow}\label{subsec:airflow}
We will use Airflow for our DataOps pipelines and to integrate our kubeflow pipelines.
We can use an Airflow DAG to trigger our Kubeflow pipelines and have a complete DataOps/MLOps pipeline while still allow respective teams
to benefit from other tools.

\subsection{Kubeflow}\label{subsec:kubeflow}
We will mainly use the storage solutions and pipelines offered by Kubeflow.
But by using kubeflow pipelines we could easily add features during the development if required by the Data or Model engineers.

\subsection{ArgoCD}\label{subsec:argocd}
ArgoCD will be used to implement our GitOps strategy and deploy our application, models, airflow and kubeflow.
We used Argo rollouts for canary deployment strategies.

\input{chapters/thesis/sections/infra}
\input{chapters/thesis/sections/roles}
\input{chapters/thesis/sections/workflow}


\section{Conclusion}\label{sec:conclusion}
Airflow pipelines and kubeflow pipelines to easily integrate DataOps and MLOps pipelines together to reach more automation as the project gains maturity.
